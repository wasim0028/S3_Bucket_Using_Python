# -*- coding: utf-8 -*-
"""Cloud.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1EXaIJQ1trfM7XcfbJHG9_DJtQ9bX0StI

# **Upload,Read And Write And Download Files From S3 bucket**
"""

!pip install boto3

# create csv file and upload it in S3
# Read the S3 bucket and read the csv files
# Download the csv files
import boto3
import pandas as pd

s3 = boto3.resource(
    service_name = 's3',
    region_name = 'us-east-1',
    aws_access_key_id = 'your_bucket_access_key',
    aws_secret_access_key = 'your_bucket_secret_access_key'
)

#print out bucket names
for bucket in s3.buckets.all():
  print(bucket.name)

!pip install s3fs

import os
os.environ["AWS_DEFAULT_REGION"] = 'us-east-1'
os.environ["AWS_ACCESS_KEY_ID"] = 'your_bucket_access_key'
os.environ["AWS_SECRET_ACCESS_KEY"] = 'your_bucket_secret_access_key'

import pandas as pd
# Make dataframes
foo = pd.DataFrame({'x': [1, 2, 3], 'y':['a', 'b', 'c']})
bar = pd.DataFrame({'x': [10, 20, 30], 'y':['aa', 'bb', 'cc']})

# Save to csv
foo.to_csv('foo.csv')
bar.to_csv('boo.csv')

# Upload files to S3 bucket
s3.Bucket('google-colab-1').upload_file(Filename = 'foo.csv', Key = 'foo1.csv')
s3.Bucket('google-colab-1').upload_file(Filename = 'boo.csv', Key = 'boo2.csv')

for obj in s3.Bucket('google-colab-1').objects.all():
  print(obj)

s3.Bucket('google-colab-1').upload_file(Filename = 'Capture.PNG', Key = 'Ticket')

# Load csv file directly into python
obj = s3.Bucket('google-colab-1').Object('foo.csv').get()
foo = pd.read_csv(obj['Body'], index_col=0)

type(foo)

foo

# Download file and read from discribe
s3.Bucket('lambdasam-12823c29e2-us-east-1').download_file(Key = 'template.yaml', Filename = 'foo3.yaml')

# create bucket using python
#import boto3
#s3 = boto3.client('s3')
#s3.create_bucket(Bucket='my-bucket-google-colab-1')
# Create bucket

s3_client = boto3.client('s3', region_name='ap-south-1')
location = {'LocationConstraint': 'ap-south-1'}
s3_client.create_bucket(Bucket='my-bucket-google-colab-2',CreateBucketConfiguration=location)

# Delete one file from the S3 bucket
import boto3

client = boto3.client('s3')
client.delete_object(Bucket='google-colab-1', Key='Ticket')

# Deleting multiple files from the S3 bucket
import boto3
from boto3.session import Session
session = Session(aws_access_key_id= 'your_bucket_access_key',
                 aws_secret_access_key='your_bucket_secret_access_key')

s3_client = session.client('s3')
s3_resource = session.resource('s3')
my_bucket = s3_resource.Bucket("google-colab-1")

response = my_bucket.delete_objects(
    Delete={
        'Objects': [
            {
                'Key': "boo.csv"   # the_name of_your_file
            },
            {
                'Key': "boo2.csv"
            }
        ]
    }
)

# Delete files from bucket
def delete_s3_bucket():
    """
    This function deletes bucket from S3
    :return: None
    """
    s3_client = boto3.client("s3")

    bucket_name = "google-colab-1"

    response = s3_client.delete_bucket(Bucket=bucket_name)
    pprint(response)
